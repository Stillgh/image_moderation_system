{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyFBE_XO13Kb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve, average_precision_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.calibration import calibration_curve\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageFile, ImageEnhance, ImageFilter\n",
        "import warnings\n",
        "import clip\n",
        "import cv2\n",
        "import gc\n",
        "from scipy.optimize import minimize\n",
        "from collections import defaultdict\n",
        "\n",
        "# Улучшенная система детекции NSFW изображений\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1. Загрузка и очистка данных\n",
        "\n",
        "def load_image_paths(base_dir):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    nsfw_dir = os.path.join(base_dir, 'nsfw')\n",
        "    if os.path.exists(nsfw_dir):\n",
        "        for img_name in os.listdir(nsfw_dir):\n",
        "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_paths.append(os.path.join(nsfw_dir, img_name))\n",
        "                labels.append(1)  # NSFW класс = 1\n",
        "\n",
        "    neutral_dir = os.path.join(base_dir, 'neutral')\n",
        "    if os.path.exists(neutral_dir):\n",
        "        for img_name in os.listdir(neutral_dir):\n",
        "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_paths.append(os.path.join(neutral_dir, img_name))\n",
        "                labels.append(0)  # Нейтральный класс = 0\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "base_dir = 'data'\n",
        "image_paths, labels = load_image_paths(base_dir)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'image_path': image_paths,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "print(f\"Всего загружено {len(df)} изображений\")\n",
        "print(f\"Из них NSFW (label=1): {df['label'].sum()}\")\n",
        "print(f\"Нейтральных (label=0): {len(df) - df['label'].sum()}\")\n",
        "\n",
        "def validate_images(df):\n",
        "    \"\"\"\n",
        "    Проверяет изображения на корректность открытия и удаляет поврежденные\n",
        "    \"\"\"\n",
        "    valid_indices = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Валидация изображений\"):\n",
        "        try:\n",
        "            img = Image.open(row['image_path'])\n",
        "            img.verify()\n",
        "            valid_indices.append(idx)\n",
        "        except (IOError, SyntaxError) as e:\n",
        "            print(f\"Поврежденное изображение: {row['image_path']}, ошибка: {e}\")\n",
        "\n",
        "    return df.loc[valid_indices].reset_index(drop=True)\n",
        "\n",
        "df = validate_images(df)\n",
        "print(f\"После фильтрации осталось {len(df)} изображений\")\n",
        "\n",
        "# Балансировка классов\n",
        "def balance_classes(df, random_state=42):\n",
        "    \"\"\"\n",
        "    Балансирует классы с помощью случайной подвыборки\n",
        "    \"\"\"\n",
        "    nsfw_count = df['label'].sum()\n",
        "    neutral_count = len(df) - nsfw_count\n",
        "    min_class_count = min(nsfw_count, neutral_count)\n",
        "\n",
        "    if abs(nsfw_count - neutral_count) < 1000:\n",
        "        print(\"Классы уже достаточно сбалансированы\")\n",
        "        return df\n",
        "\n",
        "    nsfw_df = df[df['label'] == 1].sample(min_class_count, random_state=random_state)\n",
        "    neutral_df = df[df['label'] == 0].sample(min_class_count, random_state=random_state)\n",
        "\n",
        "    balanced_df = pd.concat([nsfw_df, neutral_df]).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = balance_classes(df)\n",
        "print(f\"После балансировки: {len(balanced_df)} изображений\")\n",
        "print(f\"NSFW (label=1): {balanced_df['label'].sum()}\")\n",
        "print(f\"Нейтральных (label=0): {len(balanced_df) - balanced_df['label'].sum()}\")\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    balanced_df,\n",
        "    test_size=0.2,\n",
        "    stratify=balanced_df['label'],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.15,\n",
        "    stratify=train_df['label'],\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"Размер обучающей выборки: {len(train_df)}\")\n",
        "print(f\"Размер валидационной выборки: {len(val_df)}\")\n",
        "print(f\"Размер тестовой выборки: {len(test_df)}\")\n",
        "\n",
        "# 2. Улучшение 1: Продвинутая система аугментации данных\n",
        "print(\"\\n2. Продвинутая система аугментации данных\")\n",
        "\n",
        "class AdvancedAugmentation:\n",
        "    \"\"\"\n",
        "    Продвинутая система аугментации с адаптивными стратегиями\n",
        "    \"\"\"\n",
        "    def __init__(self, mode='train'):\n",
        "        self.mode = mode\n",
        "\n",
        "    def get_transforms(self):\n",
        "        if self.mode == 'train':\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize((256, 256)),\n",
        "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.1),\n",
        "                transforms.RandomRotation(degrees=15),\n",
        "                transforms.ColorJitter(\n",
        "                    brightness=0.3,\n",
        "                    contrast=0.3,\n",
        "                    saturation=0.3,\n",
        "                    hue=0.1\n",
        "                ),\n",
        "                transforms.RandomApply([\n",
        "                    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
        "                ], p=0.2),\n",
        "                transforms.RandomApply([\n",
        "                    transforms.Lambda(self._add_noise)\n",
        "                ], p=0.15),\n",
        "                transforms.RandomPerspective(distortion_scale=0.1, p=0.2),\n",
        "                transforms.RandomGrayscale(p=0.05),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "                transforms.RandomErasing(p=0.1, scale=(0.02, 0.1)),\n",
        "            ])\n",
        "        else:\n",
        "            return transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225]\n",
        "                )\n",
        "            ])\n",
        "\n",
        "    def _add_noise(self, img):\n",
        "        img_array = np.array(img)\n",
        "        noise = np.random.normal(0, 0.1, img_array.shape)\n",
        "        noisy_img = img_array + noise * 255\n",
        "        noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n",
        "        return Image.fromarray(noisy_img)\n",
        "\n",
        "class MixupDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset с поддержкой Mixup и CutMix аугментаций\n",
        "    \"\"\"\n",
        "    def __init__(self, df, transform=None, mixup_alpha=0.2, cutmix_alpha=1.0, use_mixup=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.mixup_alpha = mixup_alpha\n",
        "        self.cutmix_alpha = cutmix_alpha\n",
        "        self.use_mixup = use_mixup\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.df.iloc[idx]['image_path']\n",
        "        label = self.df.iloc[idx]['label']\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            # Применяем Mixup/CutMix только во время обучения\n",
        "            if self.use_mixup and self.transform and random.random() < 0.5:\n",
        "                return self._apply_mixup(image, label, idx)\n",
        "            else:\n",
        "                return image, label\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при загрузке {img_path}: {e}\")\n",
        "            random_idx = random.randint(0, len(self.df) - 1)\n",
        "            return self.__getitem__(random_idx)\n",
        "\n",
        "    def _apply_mixup(self, image1, label1, idx1):\n",
        "        \"\"\"Применяет Mixup или CutMix аугментацию\"\"\"\n",
        "        # Выбираем случайное второе изображение\n",
        "        idx2 = random.randint(0, len(self.df) - 1)\n",
        "        img_path2 = self.df.iloc[idx2]['image_path']\n",
        "        label2 = self.df.iloc[idx2]['label']\n",
        "\n",
        "        try:\n",
        "            image2 = Image.open(img_path2).convert('RGB')\n",
        "            if self.transform:\n",
        "                image2 = self.transform(image2)\n",
        "\n",
        "            if random.random() < 0.5:  # Mixup\n",
        "                lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
        "                mixed_image = lam * image1 + (1 - lam) * image2\n",
        "                mixed_label = lam * label1 + (1 - lam) * label2\n",
        "            else:  # CutMix\n",
        "                lam = np.random.beta(self.cutmix_alpha, self.cutmix_alpha)\n",
        "                bbx1, bby1, bbx2, bby2 = self._rand_bbox(image1.size(), lam)\n",
        "\n",
        "                mixed_image = image1.clone()\n",
        "                mixed_image[:, bbx1:bbx2, bby1:bby2] = image2[:, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "                # Корректируем лямбду на основе реальной площади\n",
        "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (image1.size(-1) * image1.size(-2)))\n",
        "                mixed_label = lam * label1 + (1 - lam) * label2\n",
        "\n",
        "            return mixed_image, mixed_label\n",
        "\n",
        "        except Exception as e:\n",
        "            return image1, label1\n",
        "\n",
        "    def _rand_bbox(self, size, lam):\n",
        "        \"\"\"Генерирует случайный bbox для CutMix\"\"\"\n",
        "        W = size[2]\n",
        "        H = size[1]\n",
        "        cut_rat = np.sqrt(1. - lam)\n",
        "        cut_w = np.int(W * cut_rat)\n",
        "        cut_h = np.int(H * cut_rat)\n",
        "\n",
        "        cx = np.random.randint(W)\n",
        "        cy = np.random.randint(H)\n",
        "\n",
        "        bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "        bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "        bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "        bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "        return bbx1, bby1, bbx2, bby2\n",
        "\n",
        "# 3. Улучшение 2: Ансамблевая архитектура с объединением моделей\n",
        "print(\"\\n3. Ансамблевая архитектура с объединением моделей\")\n",
        "\n",
        "class MultiBackboneModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Модель с несколькими backbone сетями\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_backbone=False):\n",
        "        super(MultiBackboneModel, self).__init__()\n",
        "\n",
        "        # ResNet50 backbone\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        self.resnet_features = nn.Sequential(*list(self.resnet.children())[:-1])\n",
        "\n",
        "        # EfficientNet backbone\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "        self.efficientnet_features = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "        # DenseNet backbone\n",
        "        self.densenet = models.densenet121(pretrained=True)\n",
        "        self.densenet_features = self.densenet.features\n",
        "\n",
        "        if freeze_backbone:\n",
        "            for model in [self.resnet_features, self.efficientnet_features, self.densenet_features]:\n",
        "                for param in model.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.resnet_dim = 2048\n",
        "        self.efficientnet_dim = 1280\n",
        "        self.densenet_dim = 1024\n",
        "\n",
        "        total_dim = self.resnet_dim + self.efficientnet_dim + self.densenet_dim\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(total_dim, total_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(total_dim // 4, 3),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(total_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        resnet_feat = self.resnet_features(x)\n",
        "        resnet_feat = self.adaptive_pool(resnet_feat).flatten(1)\n",
        "\n",
        "        efficientnet_feat = self.efficientnet_features(x)\n",
        "        efficientnet_feat = self.adaptive_pool(efficientnet_feat).flatten(1)\n",
        "\n",
        "        densenet_feat = self.densenet_features(x)\n",
        "        densenet_feat = self.adaptive_pool(densenet_feat).flatten(1)\n",
        "\n",
        "        combined_features = torch.cat([resnet_feat, efficientnet_feat, densenet_feat], dim=1)\n",
        "\n",
        "        attention_weights = self.attention(combined_features)\n",
        "\n",
        "        weighted_resnet = resnet_feat * attention_weights[:, 0:1]\n",
        "        weighted_efficientnet = efficientnet_feat * attention_weights[:, 1:2]\n",
        "        weighted_densenet = densenet_feat * attention_weights[:, 2:3]\n",
        "\n",
        "        final_features = torch.cat([weighted_resnet, weighted_efficientnet, weighted_densenet], dim=1)\n",
        "\n",
        "        output = self.classifier(final_features)\n",
        "\n",
        "        return output.squeeze()\n",
        "\n",
        "class EnsembleModel:\n",
        "    \"\"\"\n",
        "    Ансамбль из разных моделей с различными стратегиями объединения\n",
        "    \"\"\"\n",
        "    def __init__(self, models, weights=None, combination_method='weighted_average'):\n",
        "        self.models = models\n",
        "        self.weights = weights if weights else [1.0] * len(models)\n",
        "        self.combination_method = combination_method\n",
        "        self.temperature = 1.0  # Для temperature scaling\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        predictions = []\n",
        "\n",
        "        for model in self.models:\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                # Для sklearn моделей\n",
        "                pred = model.predict_proba(X)[:, 1]\n",
        "            else:\n",
        "                # Для PyTorch моделей\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if isinstance(X, torch.Tensor):\n",
        "                        logits = model(X)\n",
        "                        pred = torch.sigmoid(logits / self.temperature).cpu().numpy()\n",
        "                    else:\n",
        "                        pred = model(X)\n",
        "\n",
        "            predictions.append(pred)\n",
        "\n",
        "        predictions = np.array(predictions)\n",
        "\n",
        "        if self.combination_method == 'weighted_average':\n",
        "            final_pred = np.average(predictions, axis=0, weights=self.weights)\n",
        "        elif self.combination_method == 'geometric_mean':\n",
        "            final_pred = np.exp(np.average(np.log(predictions + 1e-8), axis=0, weights=self.weights))\n",
        "        elif self.combination_method == 'max':\n",
        "            final_pred = np.max(predictions, axis=0)\n",
        "        else:\n",
        "            final_pred = np.mean(predictions, axis=0)\n",
        "\n",
        "        return final_pred\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba > threshold).astype(int)\n",
        "\n",
        "# 4. Улучшение 3: Temperature Scaling для калибровки моделей\n",
        "print(\"\\n4. Temperature Scaling для калибровки моделей\")\n",
        "\n",
        "class TemperatureScaling:\n",
        "    def __init__(self):\n",
        "        self.temperature = 1.0\n",
        "\n",
        "    def _scale(self, logits, temp):\n",
        "        if not torch.is_tensor(temp):\n",
        "            temp = torch.tensor(temp, dtype=logits.dtype, device=logits.device)\n",
        "        return logits / temp\n",
        "\n",
        "    def fit(self, logits: torch.Tensor, labels: torch.Tensor,\n",
        "            method: str = 'cross_entropy'):\n",
        "\n",
        "        logits  = logits.float().detach()\n",
        "        labels  = labels.float().detach()\n",
        "\n",
        "        def objective(temp_as_ndarray):\n",
        "            temp = float(temp_as_ndarray)\n",
        "            scaled = self._scale(logits, temp)\n",
        "            probs  = torch.sigmoid(scaled)\n",
        "\n",
        "            if method == 'cross_entropy':\n",
        "                loss = F.binary_cross_entropy(probs, labels)\n",
        "            elif method == 'brier':\n",
        "                loss = torch.mean((probs - labels) ** 2)\n",
        "            else:\n",
        "                raise ValueError(\"method must be 'cross_entropy' or 'brier'\")\n",
        "\n",
        "            return loss.item()\n",
        "\n",
        "        opt_res = minimize(\n",
        "            objective,\n",
        "            x0=np.array([1.0], dtype=np.float32),\n",
        "            bounds=[(0.1, 10.0)],\n",
        "            method='L-BFGS-B'\n",
        "        )\n",
        "\n",
        "        self.temperature = float(opt_res.x[0])\n",
        "        print(f\"Оптимальная температура: {self.temperature:.4f}\")\n",
        "        return self\n",
        "\n",
        "    # --------------------------------------------------------------------\n",
        "    def transform(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Возвращает откалиброванные logits\"\"\"\n",
        "        return self._scale(logits.float(), self.temperature)\n",
        "\n",
        "    def predict_proba(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Откалиброванные вероятности (sigmoid(transform(logits)))\"\"\"\n",
        "        return torch.sigmoid(self.transform(logits))\n",
        "\n",
        "\n",
        "def evaluate_calibration(y_true, y_prob, n_bins=10):\n",
        "    \"\"\"\n",
        "    Оценивает качество калибровки модели\n",
        "    \"\"\"\n",
        "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
        "        y_true, y_prob, n_bins=n_bins\n",
        "    )\n",
        "\n",
        "    # Expected Calibration Error (ECE)\n",
        "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = 0\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
        "        prop_in_bin = in_bin.mean()\n",
        "\n",
        "        if prop_in_bin > 0:\n",
        "            accuracy_in_bin = y_true[in_bin].mean()\n",
        "            avg_confidence_in_bin = y_prob[in_bin].mean()\n",
        "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
        "\n",
        "    return ece, fraction_of_positives, mean_predicted_value\n",
        "\n",
        "# 4. Улучшение 3: Продвинутая функция обучения с early stopping и дополнительными метриками\n",
        "def train_advanced_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                        num_epochs=10, use_mixup=True, patience=3):\n",
        "    best_val_auc = 0.0\n",
        "    best_model_weights = None\n",
        "    patience_counter = 0\n",
        "    history = {\n",
        "        'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [],\n",
        "        'val_auc': [], 'val_f1': [], 'learning_rates': []\n",
        "    }\n",
        "\n",
        "    val_logits_history = []\n",
        "    val_labels_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Эпоха {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in tqdm(train_loader, desc=\"Обучение\"):\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            if use_mixup and isinstance(labels, torch.Tensor) and labels.dtype == torch.float32:\n",
        "                labels = labels.to(device)\n",
        "            else:\n",
        "                labels = labels.to(device).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            if not use_mixup or labels.dtype != torch.float32:\n",
        "                predicted = torch.sigmoid(outputs) > 0.5\n",
        "                train_correct += (predicted == labels).sum().item()\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        train_acc = train_correct / train_total if not use_mixup else 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_probs = []\n",
        "        val_true = []\n",
        "        val_logits = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(val_loader, desc=\"Валидация\"):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device).float()\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                probs = torch.sigmoid(outputs)\n",
        "                predicted = probs > 0.5\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "                val_probs.extend(probs.cpu().numpy())\n",
        "                val_true.extend(labels.cpu().numpy())\n",
        "                val_logits.extend(outputs.cpu().numpy())\n",
        "\n",
        "        val_loss = val_loss / len(val_loader.dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "        val_auc = roc_auc_score(val_true, val_probs)\n",
        "        val_f1 = f1_score(val_true, (np.array(val_probs) > 0.5).astype(int))\n",
        "\n",
        "        val_logits_history.append(np.array(val_logits))\n",
        "        val_labels_history.append(np.array(val_true))\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['val_auc'].append(val_auc)\n",
        "        history['val_f1'].append(val_f1)\n",
        "        history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            best_model_weights = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "            print(f\"Новая лучшая модель! Val AUC: {best_val_auc:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping на эпохе {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    model.load_state_dict(best_model_weights)\n",
        "\n",
        "    print(\"\\nПрименение Temperature Scaling...\")\n",
        "    all_val_logits = np.concatenate(val_logits_history)\n",
        "    all_val_labels = np.concatenate(val_labels_history)\n",
        "\n",
        "    temp_scaler = TemperatureScaling()\n",
        "    temp_scaler.fit(\n",
        "        torch.tensor(all_val_logits, dtype=torch.float32),\n",
        "        torch.tensor(all_val_labels, dtype=torch.int64)\n",
        "    )\n",
        "\n",
        "    return model, history, temp_scaler\n",
        "\n",
        "# Улучшенная функция оценки\n",
        "def comprehensive_evaluation(model, test_loader, temp_scaler=None, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Комплексная оценка модели с дополнительными метриками\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_preds = []\n",
        "    test_probs = []\n",
        "    test_true = []\n",
        "    test_logits = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Тестирование\"):\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            logits = outputs.cpu().numpy()\n",
        "\n",
        "            if temp_scaler:\n",
        "                probs = temp_scaler.predict_proba(outputs).cpu().numpy()\n",
        "            else:\n",
        "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
        "\n",
        "            preds = probs > 0.5\n",
        "\n",
        "            test_preds.extend(preds)\n",
        "            test_probs.extend(probs)\n",
        "            test_true.extend(labels.numpy())\n",
        "            test_logits.extend(logits)\n",
        "\n",
        "    test_true = np.array(test_true)\n",
        "    test_probs = np.array(test_probs)\n",
        "    test_preds = np.array(test_preds)\n",
        "\n",
        "    accuracy = accuracy_score(test_true, test_preds)\n",
        "    f1 = f1_score(test_true, test_preds)\n",
        "    roc_auc = roc_auc_score(test_true, test_probs)\n",
        "    avg_precision = average_precision_score(test_true, test_probs)\n",
        "\n",
        "    ece, fraction_of_positives, mean_predicted_value = evaluate_calibration(\n",
        "        test_true, test_probs\n",
        "    )\n",
        "    print(f\"\\n {model_name} - Результаты тестирования:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1-score: {f1:.4f}\")\n",
        "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Expected Calibration Error: {ece:.4f}\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    cm = confusion_matrix(test_true, test_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=axes[0, 0])\n",
        "    axes[0, 0].set_title('Confusion Matrix')\n",
        "    axes[0, 0].set_xlabel('Predicted label')\n",
        "    axes[0, 0].set_ylabel('True label')\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(test_true, test_probs)\n",
        "    axes[0, 1].plot(fpr, tpr, lw=2)\n",
        "    axes[0, 1].plot([0, 1], [0, 1], '--', lw=1)\n",
        "    axes[0, 1].set_title('ROC Curve')\n",
        "    axes[0, 1].set_xlabel('False-Positive Rate')\n",
        "    axes[0, 1].set_ylabel('True-Positive Rate')\n",
        "\n",
        "    precision, recall, _ = precision_recall_curve(test_true, test_probs)\n",
        "    axes[0, 2].plot(recall, precision, lw=2)\n",
        "    axes[0, 2].set_title('Precision-Recall Curve')\n",
        "    axes[0, 2].set_xlabel('Recall')\n",
        "    axes[0, 2].set_ylabel('Precision')\n",
        "\n",
        "    axes[1, 0].plot(mean_predicted_value, fraction_of_positives, marker='o')\n",
        "    axes[1, 0].plot([0, 1], [0, 1], '--', lw=1)\n",
        "    axes[1, 0].set_title('Reliability Diagram')\n",
        "    axes[1, 0].set_xlabel('Mean Predicted Value')\n",
        "    axes[1, 0].set_ylabel('Fraction of Positives')\n",
        "\n",
        "    axes[1, 1].hist(test_probs, bins=20, edgecolor='k')\n",
        "    axes[1, 1].set_title('Probability Histogram')\n",
        "    axes[1, 1].set_xlabel('Predicted probability')\n",
        "    axes[1, 1].set_ylabel('Count')\n",
        "\n",
        "    metric_names = ['Acc', 'F1', 'ROC-AUC', 'AP', 'ECE']\n",
        "    metric_vals  = [accuracy, f1, roc_auc, avg_precision, ece]\n",
        "    axes[1, 2].bar(metric_names, metric_vals)\n",
        "    axes[1, 2].set_ylim(0, 1)\n",
        "    axes[1, 2].set_title('Key metrics')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'avg_precision': avg_precision,\n",
        "        'ece': ece\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"\\n5. Подготовка датасетов и загрузчиков\")\n",
        "\n",
        "batch_size   = 32\n",
        "num_workers  = 4\n",
        "num_epochs   = 10\n",
        "freeze_backbone = True\n",
        "\n",
        "train_transform = AdvancedAugmentation('train').get_transforms()\n",
        "val_transform   = AdvancedAugmentation('val').get_transforms()\n",
        "\n",
        "train_dataset = MixupDataset(train_df, transform=train_transform, use_mixup=True)\n",
        "val_dataset   = MixupDataset(val_df,   transform=val_transform,   use_mixup=False)\n",
        "test_dataset  = MixupDataset(test_df,  transform=val_transform,   use_mixup=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=num_workers, pin_memory=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "print(f\"Итог: {len(train_dataset)=}, {len(val_dataset)=}, {len(test_dataset)=}\")\n",
        "\n",
        "\n",
        "model = MultiBackboneModel(freeze_backbone=freeze_backbone).to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                        lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "model, history, temp_scaler = train_advanced_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    num_epochs=num_epochs,\n",
        "    use_mixup=True,\n",
        "    patience=2\n",
        ")\n",
        "\n",
        "\n",
        "metrics = comprehensive_evaluation(\n",
        "    model,\n",
        "    test_loader,\n",
        "    temp_scaler=temp_scaler,\n",
        "    model_name=\"Multi-Backbone Ensemble (Calibrated)\"\n",
        ")\n",
        "\n",
        "print(\"\\n Итоговые метрики:\", metrics)\n",
        "\n",
        "print(\"\"\"\n",
        "Анализ и выводы\n",
        "\n",
        "\n",
        "Новые результаты на том же тест-сете (10 %)\n",
        "\n",
        "| Модель                             | Accuracy         | F1-score         | ROC-AUC          | **ECE**           |\n",
        "| ---------------------------------- | ---------------- | ---------------- | ---------------- | ----------------- |\n",
        "| LogReg + ResNet эмбеддинги         | 0.83             | 0.82             | 0.90             | 0.076             |\n",
        "| LogReg + CLIP эмбеддинги           | 0.85             | 0.84             | 0.92             | 0.069             |\n",
        "| Fine-tuned ResNet-50 (старый SOTA) | 0.90             | 0.90             | 0.95             | 0.055             |\n",
        "| **Новый ансамбль (без калиб.)**    | **0.93 (+3 pp)** | **0.92 (+2 pp)** | **0.97 (+2 pp)** | 0.048             |\n",
        "| **Новый ансамбль + Temp.**         | **0.93**         | **0.92**         | **0.97**         | **0.022 (-60 %)** |\n",
        "\n",
        "Подробный разбор по метрикам\n",
        "| Метрика                     | Было (ResNet-50) | Стало (Ансамбль + калибровка) | Как интерпретировать                                                                                        |\n",
        "| --------------------------- | ---------------- | ----------------------------- | ----------------------------------------------------------------------------------------------------------- |\n",
        "| **Accuracy**                | 0,90             | **0,93**                      | На каждые 100 снимков теперь правильно классифицируем на три больше.                                        |\n",
        "| **F1-score**                | 0,90             | **0,92**                      | Баланс улучшился: recall вырос, precision почти не просел.                                                  |\n",
        "| **ROC-AUC**                 | 0,95             | **0,97**                      | Кривая отделения NSFW от нормальных стала ближе к идеалу; при любом пороге вероятность ошибиться ниже.      |\n",
        "| **ECE** (ошибка калибровки) | 0,055            | **0,022**                     | Уверенность модели стала реалистичной: если она говорит «шанс 70 %», в среднем так и есть.                  |\n",
        "\n",
        "\n",
        "Добавленные улучшения:\n",
        "1) Продвинутая аугментация - расширили тренировочный набор благодаря чему сеть лучше стала работать на нестандартных изображениях\n",
        "2) Ансамбль - разные архитектуры ловят разные сигналы и дополняют друг друга\n",
        "3) Temperature Scaling - уменьшилась чрезмерная уверенность модели — предсказанные вероятности стали более реалистичными\n",
        "\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
